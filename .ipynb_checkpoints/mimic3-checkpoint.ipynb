{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   D  id\n",
      "0  6   1\n",
      "1  4   2\n",
      "2  9   2\n",
      "3  4   1\n",
      "4  8   3\n",
      "5  3   3\n",
      "6  4   4\n",
      "7  6   6\n",
      "8  7   6\n",
      "9  9   6\n",
      "   D  id  Dscal\n",
      "0  6   1   True\n",
      "1  4   2  False\n",
      "2  9   2   True\n",
      "3  4   1  False\n",
      "4  8   3  False\n",
      "5  3   3   True\n",
      "6  4   4  False\n",
      "7  6   6   True\n",
      "8  7   6  False\n",
      "9  9   6   True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No axis named Dscal for object type <class 'pandas.core.frame.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-f96cb83d8b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dscal'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'D'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dscal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Dscal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dscal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dscal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#df2 =  df.unstack().reset_index()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[0;32m   4411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mby\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4412\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4413\u001b[1;33m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4414\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[0;32m   4415\u001b[0m                        \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_axis_number\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         raise ValueError('No axis named {0} for object type {1}'\n\u001b[1;32m--> 353\u001b[1;33m                          .format(axis, type(self)))\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No axis named Dscal for object type <class 'pandas.core.frame.DataFrame'>"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame({'A':[1,2,2,1,3,3,4,6,6],'B':[2,2,9,4,3,56,4,2,2],'C':[2,4,6,8,7,2,0,3,9],'D':[6,4,9,4,8,3,4,6,7]})\n",
    "df = pd.DataFrame({'id':[1,2,2,1,3,3,4,6,6,6],'D':[6,4,9,4,8,3,4,6,7,9]})\n",
    "print df\n",
    "df['Dscal'] = df['D']%3==0\n",
    "print df\n",
    "print df[['Dscal','id']].groupby('id','Dscal').count()\n",
    "df2 =  df['Dscal'].groupby([df['id'],df['Dscal']]).count().unstack().reset_index()\n",
    "#df2 =  df.unstack().reset_index()\n",
    "print df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_id,id转换为字符串\n",
    "loginData=pd.read_csv(\"input/t_login.csv\",dtype={'log_id':str,'id':str})\n",
    "loginTestData=pd.read_csv(\"input/t_login_test.csv\",dtype={'log_id':str,'id':str})\n",
    "tradeData=pd.read_csv(\"input/trade_2_login.csv\",dtype={'id':str,'login_id':str})\n",
    "tradeTestData=pd.read_csv(\"input/t_trade_test_2_login.csv\",dtype={'id':str,'login_id':str})\n",
    "loginTestData=loginTestData.append(loginData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到DataFrame中的无重复的id\n",
    "def getUserIDFromDataFrame(dataFrame):\n",
    "    return pd.DataFrame({'id':dataFrame['id'].unique()})\n",
    "\t\n",
    "#得到登录中所用时间为整秒的计算\n",
    "def getTimelongScale(loginData,test):\n",
    "    #计算是否为整秒\n",
    "    loginData['timeScale']=(loginData['timelong']%1000==0)\n",
    "    #分别为整数 ，不为整数的次数\n",
    "    gd=loginData['timeScale'].groupby([loginData['id'],loginData['timeScale']])\n",
    "    tmp=gd.count().unstack().reset_index()\n",
    "    tmp=tmp.fillna(0)\n",
    "    test=pd.merge(tmp,test,on='id',how='inner')\n",
    "#   print(test.head(2))\n",
    "    del loginData['timeScale'],tmp\n",
    "    return test\n",
    "\n",
    "\n",
    "#得到某列的分组不同的个数\n",
    "def getCountsByColumnName(loginData,test,columnName):\n",
    "    num=loginData[['id',columnName]].groupby(loginData['id'])[columnName].nunique()\n",
    "    t=pd.DataFrame(num)\n",
    "    t.insert(0, 'id', num.index)\n",
    "    t.columns=['id',columnName+'_Counts']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "#得到某列的分组平均值\n",
    "def getMeanByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.mean().reset_index()\n",
    "    t.columns=['id',columnName+'_Mean']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "#得到某列的分组最小值\n",
    "def getMinByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.min().reset_index()\n",
    "    t.columns=['id',columnName+'_min']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "#得到某列的分组最大值\n",
    "def getMaxByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.max().reset_index()\n",
    "    t.columns=['id',columnName+'_max']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "\n",
    "#得到某列的分组方差\n",
    "def getVarByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.var().reset_index()\n",
    "    t.columns=['id',columnName+'_var']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "#得到某列的分组标准差\n",
    "def getStdByColumnName(loginData,test,columnName):\n",
    "    t=loginData[columnName].groupby(loginData['id'])\n",
    "    t=t.std().reset_index()\n",
    "    t.columns=['id',columnName+'_std']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    return test\n",
    "\n",
    "#得到对登录result的处理\n",
    "def getLoginResultCount(loginData,test):\n",
    "    r31=loginData[loginData['result']!=1]\n",
    "    t=r31['result'].groupby(r31['id'])\n",
    "    t=t.count().reset_index()\n",
    "    t.columns=['id','result_no1_Counts']\n",
    "    test=pd.merge(t,test,on='id',how='inner')\n",
    "    test=test.fillna(0)\n",
    "    return test\n",
    "\n",
    "\n",
    "#每次登陆的时间段的比例\n",
    "def getLoginHoursSacle(tradeData,loginData):\n",
    "    loginData['tx']=pd.to_datetime(loginData['time'])\n",
    "    loginData['date']=loginData['tx'].dt.date\n",
    "    loginData['hour']=loginData['tx'].dt.hour\n",
    "    #每天在同一个小时内多次登陆只算一次\n",
    "    un=loginData[['id','date','hour','log_id']].groupby(by=['id','date','hour'])['log_id'].nunique()\n",
    "    un=un.reset_index()\n",
    "    tmp=un[['id','hour']].groupby(by='id')['hour'].agg([np.mean,np.std,np.var]).reset_index()\n",
    "    tmp.columns=['id','l_time_mean','l_time_std','l_time_var']\n",
    "    loginDataTime=pd.merge(loginData,tmp,on='id',how='inner')\n",
    "    loginDataTime['sub_l_time_mean']=np.abs(loginDataTime['hour']-loginDataTime['l_time_mean']) \n",
    "    tradeData=pd.merge(tradeData,loginDataTime[['log_id','l_time_mean','l_time_std','l_time_var','sub_l_time_mean']],\n",
    "                       left_on='login_id',right_on='log_id',how='inner')\n",
    "    del tradeData['log_id']\n",
    "    del loginDataTime\n",
    "    del loginData['tx'],loginData['date'],loginData['hour'],un,tmp\n",
    "    #因为线上分数有点低，暂时删除以下\n",
    "    del tradeData['l_time_mean'],tradeData['l_time_std'],tradeData['l_time_var']\n",
    "    return tradeData\n",
    "# getLoginHoursSacle(tradeData,loginData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#处理交易数据\n",
    "def tradeDataInit(tradeData,loginData):\n",
    "    tradeData=tradeData.copy()\n",
    "    tradeData=getLoginHoursSacle(tradeData,loginData)\n",
    "    #构建一个小时的\n",
    "    tradeData['hours']=tradeData['time'].str.extract('\\\\s(\\\\d\\\\d):')\n",
    "    tradeData['hours']=tradeData['hours'].astype('int')\n",
    "    tradeData=pd.merge(tradeData,loginData[['log_id','time']],left_on='login_id',right_on='log_id',how='inner')\n",
    "    del tradeData['log_id']\n",
    "#     tradeData['hours']=\n",
    "#     print(OneHotEncoder(sparse = False).fit_transform( tradeData[['hours']]))\n",
    "    #交易时间\n",
    "    tradeData['tx']=pd.to_datetime(tradeData['time_x'])\n",
    "    #登录时间\n",
    "    tradeData['ty']=pd.to_datetime(tradeData['time_y'])\n",
    "    #交易时间和登录时间之间的差值\n",
    "    tradeData['st']=(tradeData['tx']-tradeData['ty']).dt.seconds\n",
    "    #每次交易时间的差\n",
    "    tradeData['trade_time_sub']=tradeData[['id','tx']].sort_values(by='tx').groupby('id').diff()['tx']\n",
    "    tradeData['trade_time_sub_day']=tradeData['trade_time_sub'].dt.days\n",
    "    tradeData=getMaxByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getMeanByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getMinByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getStdByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    tradeData=getVarByColumnName(tradeData,tradeData,'trade_time_sub_day')\n",
    "    #使用的比例\n",
    "    tradeData=getTradeLoginColumScale(tradeData,loginData,'device')\n",
    "    tradeData=getTradeLoginColumScale(tradeData,loginData,'city')\n",
    "    \n",
    "    del tradeData['tx'],tradeData['ty'],tradeData['time_y'],tradeData['login_id'],tradeData['trade_time_sub']\n",
    "    tradeData.rename(columns={'time_x':'time'},inplace=True)\n",
    "    return tradeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTradeLoginColumScale(tradeData,loginData,cname):\n",
    "    #计算cname列上分别使用的个数\n",
    "    c_count=loginData[['id',cname,'log_id']].groupby(['id',cname]).count()\n",
    "    c_count=c_count.reset_index()\n",
    "    c_count.columns=['id',cname,cname+'_count_tmp']\n",
    "    #使用的总个数\n",
    "    c_sum=c_count[['id',cname+'_count_tmp']].groupby('id').sum().reset_index()\n",
    "    c_sum.columns=['id',cname+'_count_sum']\n",
    "    c=pd.merge(c_count,c_sum,on='id',how='inner')\n",
    "    #使用的比例\n",
    "    c[cname+'_scale']=c[cname+'_count_tmp']/c[cname+'_count_sum']\n",
    "    del c[cname+'_count_tmp'],c[cname+'_count_sum']\n",
    "    c=pd.merge(loginData[['log_id','id',cname]],c,on=['id',cname],how='inner')\n",
    "    tradeData=pd.merge(tradeData,c[['log_id',cname+'_scale']],left_on='login_id',right_on='log_id',how='inner')\n",
    "    del c,tradeData['log_id']\n",
    "    return tradeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把处理后的登录数据和交易数据内连接\n",
    "def createAllData(test,tradeData):\n",
    "    tmp=pd.merge(test,tradeData,on='id',how='inner')\n",
    "#     tmp=tmp.fillna(0)\n",
    "#     print(tmp.info())\n",
    "    return tmp\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "#评估函数\n",
    "def rocJdScore(*args):\n",
    "    from sklearn import metrics\n",
    "    return metrics.make_scorer(fbeta_score,beta=0.1, greater_is_better=True)(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPipe():\n",
    "    # 下面，我要用逻辑回归拟合模型，并用标准化和PCA（30维->2维）对数据预处理，用Pipeline类把这些过程链接在一起\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import xgboost as xgb\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "    #xgb的配置\n",
    "    xgbFier = XGBClassifier(\n",
    "             learning_rate =0.3,\n",
    "             n_estimators=1000,\n",
    "             max_depth=5,\n",
    "             min_child_weight=1,\n",
    "             gamma=0,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=0.8,\n",
    "             objective= 'binary:logistic',\n",
    "             nthread=2,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27,\n",
    "             silent=0\n",
    "    )\n",
    "    # 用StandardScaler和PCA作为转换器，LogisticRegression作为评估器\n",
    "    estimators = [\n",
    "#         ('scl', StandardScaler()), \n",
    "#                   ('pca', PCA(n_components=2)), \n",
    "#                    ('rf', RandomForestClassifier(random_state=1,\n",
    "#                                                  max_depth= 50,\n",
    "#                                                  min_samples_leaf= 3,\n",
    "#                                                  min_samples_split= 10,\n",
    "#                                                  n_estimators= 20,\n",
    "#                                                 )),\n",
    "#                   ('dtc',DecisionTreeClassifier(criterion='entropy')),\n",
    "                                    ('xgb',xgbFier),\n",
    "#                   ('lr', LogisticRegression())\n",
    "                 ]\n",
    "    # estimators = [ ('clf', RandomForestClassifier(random_state=1))]\n",
    "    # Pipeline类接收一个包含元组的列表作为参数，每个元组的第一个值为任意的字符串标识符，\n",
    "    #比如：我们可以通过pipe_lr.named_steps['pca']来访问PCA组件;第二个值为scikit-learn的转换器或评估器\n",
    "    pipe_lr = Pipeline(estimators)\n",
    "    return pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到训练用的测试集元组（x，y）\n",
    "def getTrainData(isUndersample=False):\n",
    "    allData=transferData(loginData,tradeData)\n",
    "    if(isUndersample):\n",
    "        #进行undersample的处理\n",
    "        number_records_fraud=len(allData[allData['is_risk']==1]) #有风险的个数\n",
    "        fraud_indices=np.array(allData[allData['is_risk']==1].index) #有风险的index\n",
    "        normal_indices=allData[allData['is_risk']==0].index\n",
    "        random_normal_indices=np.random.choice(normal_indices,number_records_fraud,replace=False)\n",
    "        random_normal_indices=np.array(random_normal_indices)\n",
    "        under_sample_indices=np.concatenate([fraud_indices,random_normal_indices])\n",
    "        allData=allData.iloc[under_sample_indices,:]\n",
    "    \n",
    "    x=allData.iloc[:,3:].values\n",
    "    y=allData['is_risk'].values\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成学习算法\n",
    "def jdPipeFited(pipe_lr):    \n",
    "    x,y=getTrainData(isUndersample=False)\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    # 拆分成训练集(80%)和测试集(20%)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1) \n",
    "    \n",
    "    pipe_lr.fit(x, y)\n",
    "    return pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "你可以理解为短时间内变化城市、切换IP、更换设备、三更半夜多笔交易、不在常用设备登陆、不在常在城市登陆、短时间多次登陆的后续交易\n",
    "'''\n",
    "#生成训练数据\n",
    "def transferData(loginData,tradeData):    \n",
    "    lData=getUserIDFromDataFrame(loginData)\n",
    "    #登录耗费时间段是否为整数\n",
    "    lData=getTimelongScale(loginData,lData)\n",
    "    lData=getMinByColumnName(loginData,lData,'timelong')\n",
    "    lData=getMaxByColumnName(loginData,lData,'timelong')\n",
    "    lData=getMeanByColumnName(loginData,lData,'timelong')\n",
    "    lData=getVarByColumnName(loginData,lData,'timelong')\n",
    "    lData=getStdByColumnName(loginData,lData,'timelong')\n",
    "# #     #device的个数\n",
    "#     lData=getCountsByColumnName(loginData,lData,'device')\n",
    "# #     #ip的个数\n",
    "#     lData=getCountsByColumnName(loginData,lData,'ip')\n",
    "#     lData=getCountsByColumnName(loginData,lData,'log_from')\n",
    "    \n",
    "    lData=getCountsByColumnName(loginData,lData,'type')\n",
    "#     #城市的个数\n",
    "    lData=getCountsByColumnName(loginData,lData,'city')\n",
    "    #登录的次数\n",
    "#     lData=getCountsByColumnName(loginData,lData,'log_id')\n",
    "    #平均值\n",
    "#     lData=getMeanByColumnName(loginData,lData,'device')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'log_from')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'ip')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'city')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'result')\n",
    "#     lData=getMeanByColumnName(loginData,lData,'type')\n",
    "    #result的处理\n",
    "    lData=getLoginResultCount(loginData,lData)\n",
    "    \n",
    "    tData=tradeData.copy()\n",
    "    tData=tradeDataInit(tData,loginData)\n",
    "#     tData=getLastSubTime(loginData,tData)\n",
    "    allData=createAllData(lData,tData)\n",
    "    del allData['time']\n",
    "    # get a list of columns\n",
    "    cols = list(allData)\n",
    "    if 'is_risk' in allData.columns:\n",
    "        cols.insert(0, cols.pop(cols.index('is_risk')))\n",
    "    cols.insert(0, cols.pop(cols.index('rowkey')))\n",
    "    \n",
    "    allData = allData.ix[:, cols]\n",
    "    print(allData.head(2))\n",
    "    return allData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold交叉验证\n",
    "def crossval():\n",
    "\tfrom sklearn.cross_validation import cross_val_score\n",
    "\tpipe_lr=getPipe()\n",
    "\tX_train,y_train=getTrainData(isUndersample=False)\n",
    "\t#记录程序运行时间\n",
    "\timport time \n",
    "\tstart_time = time.time()\n",
    "\tscores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=3, n_jobs=2,scoring=rocJdScore)\n",
    "\tprint(scores)\n",
    "\t# #整体预测\n",
    "\t# X_train,y_train=getTrainData(isUndersample=False)\n",
    "\t# pipe_lr\n",
    "\t#输出运行时长\n",
    "\tcost_time = time.time()-start_time\n",
    "\tprint(\"交叉验证 success!\",'\\n',\"cost time:\",cost_time,\"(s)\")\t\n",
    "\t\n",
    "#网格搜索实验\n",
    "def gridSer():\n",
    "\tfrom sklearn.grid_search import GridSearchCV\n",
    "\tparameters = {\n",
    "\t#     'rf__n_estimators': (5, 10, 20, 50),\n",
    "\t#     'rf__max_depth': (50, 150, 250),\n",
    "\t#     'rf__min_samples_split': [10, 2, 3],\n",
    "\t#     'rf__min_samples_leaf': (1, 2, 3),\n",
    "\t\t#xgb的参数\n",
    "\t\t'xgb__max_depth':(4,6),\n",
    "\t\t'xgb__learning_rate':(0.3,0.5)\n",
    "\n",
    "\t}\n",
    "\tpipe_lr=getPipe()\n",
    "\tX_train,y_train=getTrainData()\n",
    "\n",
    "\n",
    "\t#网格搜索\n",
    "\tgrid_search = GridSearchCV(pipe_lr, parameters, n_jobs=-1, verbose=1, scoring=rocJdScore)\n",
    "\tgrid_search.fit(X_train, y_train)\n",
    "\n",
    "\t#获取最优参数\n",
    "\tprint('最佳效果：%0.3f' % grid_search.best_score_)\n",
    "\tprint('最优参数：')\n",
    "\tbest_parameters = grid_search.best_estimator_.get_params()\n",
    "\tfor param_name in sorted(parameters.keys()):\n",
    "\t\tprint('\\t%s= %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\t# #预测以及分类器参数报告\n",
    "\t# predictions = grid_search.predict(X_test)\n",
    "\t# print(classification_report(y_test, predictions))\t\n",
    "\t\n",
    "#学习曲线\n",
    "def learncur():\n",
    "\tfrom sklearn.learning_curve import learning_curve\n",
    "\n",
    "\tpipe_lr = getPipe()\n",
    "\tX_train,y_train=getTrainData()\n",
    "\t# train_sizes参数指定用于生成学习曲线的训练集数量，如果是分数指的是相对数量，整数指的是绝对数量\n",
    "\ttrain_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=3,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=2,scoring=rocJdScore)\n",
    "\tprint('==='*10)\n",
    "\ttrain_mean = np.mean(train_scores, axis=1)\n",
    "\ttrain_std = np.std(train_scores, axis=1)\n",
    "\ttest_mean = np.mean(test_scores, axis=1)\n",
    "\ttest_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\tplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n",
    "\tplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "\tplt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "\tplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "\tplt.grid()\n",
    "\tplt.xlabel('Number of training samples')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.legend(loc='lower right')\n",
    "\tplt.ylim([0.0, 1.0])\n",
    "\tplt.savefig(\"curve.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  import sys\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:46: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rowkey  is_risk      id  result_no1_Counts  city_Counts  type_Counts  \\\n",
      "0  745277        0  100002                  4            2            2   \n",
      "1  745744        0  100002                  4            2            2   \n",
      "\n",
      "   timelong_std  timelong_var  timelong_Mean  timelong_max     ...      \\\n",
      "0   6499.943066  4.224926e+07   12543.111111       22119.0     ...       \n",
      "1   6499.943066  4.224926e+07   12543.111111       22119.0     ...       \n",
      "\n",
      "   trade_time_sub_day_std  trade_time_sub_day_min  trade_time_sub_day_Mean  \\\n",
      "0                     NaN                     0.0                      0.0   \n",
      "1                     NaN                     0.0                      0.0   \n",
      "\n",
      "   trade_time_sub_day_max  sub_l_time_mean  hours    st  trade_time_sub_day  \\\n",
      "0                     0.0             2.75     22  1688                 NaN   \n",
      "1                     0.0             3.75     23   234                 0.0   \n",
      "\n",
      "   device_scale  city_scale  \n",
      "0      0.333333    0.777778  \n",
      "1      0.333333    0.777778  \n",
      "\n",
      "[2 rows x 24 columns]\n",
      "   rowkey      id  result_no1_Counts  city_Counts  type_Counts  timelong_std  \\\n",
      "0  128831  100002                  5            2            2  20165.097969   \n",
      "1   52214  100006                  2            3            2   6357.865593   \n",
      "\n",
      "   timelong_var  timelong_Mean  timelong_max  timelong_min     ...      \\\n",
      "0  4.066312e+08   19534.461538       66416.0        5000.0     ...       \n",
      "1  4.042245e+07    1392.000000       29140.0           3.0     ...       \n",
      "\n",
      "   trade_time_sub_day_std  trade_time_sub_day_min  trade_time_sub_day_Mean  \\\n",
      "0                     NaN                     NaN                      NaN   \n",
      "1                     NaN                     NaN                      NaN   \n",
      "\n",
      "   trade_time_sub_day_max  sub_l_time_mean  hours     st  trade_time_sub_day  \\\n",
      "0                     NaN         7.166667     10     91                 NaN   \n",
      "1                     NaN         1.765957     23  24635                 NaN   \n",
      "\n",
      "   device_scale  city_scale  \n",
      "0      0.230769    0.846154  \n",
      "1      0.209677    0.387097  \n",
      "\n",
      "[2 rows x 23 columns]\n",
      "--------------------------\n",
      "y_pred.shape (13647L, 2L)\n",
      "[[  9.99041140e-01   9.58875462e-04]\n",
      " [  9.99982059e-01   1.79314939e-05]\n",
      " [  9.99825895e-01   1.74116140e-04]\n",
      " ..., \n",
      " [  1.00000000e+00   1.42138106e-08]\n",
      " [  4.87363040e-01   5.12636960e-01]\n",
      " [  8.53736043e-01   1.46263987e-01]]\n",
      "13647.0\n",
      "------------------------------\n",
      "t.shape (13647L,)\n",
      "[  9.58875462e-04   1.79314939e-05   1.74116140e-04 ...,   1.42138106e-08\n",
      "   5.12636960e-01   1.46263987e-01]\n"
     ]
    }
   ],
   "source": [
    "#预测\n",
    "pipe=getPipe()\n",
    "pipe=jdPipeFited(pipe)\n",
    "preData=transferData(loginTestData,tradeTestData)\n",
    "x_pred=preData.iloc[:,2:].values\n",
    "y_pred=pipe.predict_proba(x_pred)\n",
    "print '--------------------------'\n",
    "print 'y_pred.shape', y_pred.shape\n",
    "print y_pred\n",
    "\n",
    "print(np.sum(y_pred))\n",
    "t=y_pred[:,1]\n",
    "print '------------------------------'\n",
    "print 't.shape',t.shape\n",
    "print t\n",
    "t[t<0.99]=0\n",
    "t[t>=0.99]=1\n",
    "t\n",
    "y_pred=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p=pd.DataFrame(y_pred)\n",
    "subData=pd.DataFrame(preData['rowkey'])\n",
    "subData['is_risk']=p\n",
    "#之前用很多inner join，很多数据没有，都默认处理为没有风险\n",
    "subData=pd.merge(tradeTestData[['rowkey']],subData,on='rowkey',how='left')\n",
    "subData=subData.fillna(0)\n",
    "subData['is_risk']=subData['is_risk'].astype('int')\n",
    "\n",
    "#subData.to_csv('output/sub.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
